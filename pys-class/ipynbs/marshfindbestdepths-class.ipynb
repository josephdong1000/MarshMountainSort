{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "762dfabe-c35e-41b9-ad8b-43ef58460f22",
   "metadata": {},
   "source": [
    "# Find Best Depths - Class\n",
    "\n",
    "Uses `.npy.gz` data files to generate figures summarizing all electrode depths for each animal.\n",
    "\n",
    "Generate the class version of this script, to use as a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f5684fa",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook marshfindbestdepths-class.ipynb to script\n",
      "[NbConvertApp] Writing 38217 bytes to /mnt/isilon/marsh_single_unit/MarshMountainSort/pys-class/mms/core.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script marshfindbestdepths-class.ipynb --TagRemovePreprocessor.remove_cell_tags='{\"note\"}' --output-dir /mnt/isilon/marsh_single_unit/MarshMountainSort/pys-class/mms --output core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c48d73",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd2415a2",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# os.path.abspath(\"../\")\n",
    "# from mms import core, parser, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d777ff6e-0e78-4b63-864f-e867bc615899",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/isilon/marsh_single_unit/MarshMountainSort/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import dateutil\n",
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "import warnings\n",
    "import tempfile\n",
    "import time\n",
    "from textwrap import wrap\n",
    "from typing import Literal\n",
    "from pprint import pprint\n",
    "\n",
    "# import pkg_resources\n",
    "# pkg_resources.require('matplotlib==3.8.3')\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import AutoLocator\n",
    "import numpy as np\n",
    "import probeinterface as pi\n",
    "from probeinterface.plotting import plot_probe\n",
    "import pandas as pd\n",
    "\n",
    "import mountainsort5 as ms5\n",
    "from mountainsort5.util import create_cached_recording\n",
    "import spikeinterface.core as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "# import spikeinterface.sorters as ss\n",
    "import spikeinterface.postprocessing as spost\n",
    "# import spikeinterface.qualitymetrics as sqm\n",
    "# import spikeinterface.exporters as sexp\n",
    "# import spikeinterface.comparison as scmp\n",
    "# import spikeinterface.curation as scur\n",
    "import spikeinterface.sortingcomponents as sc\n",
    "# from spikeinterface.sortingcomponents.motion_interpolation import \n",
    "import spikeinterface.widgets as sw\n",
    "\n",
    "from mms import constants\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from functools import partialmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57873bb5-5353-4756-bf3f-69833fa33dbd",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be468ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_tempdir(path:str):\n",
    "    tempfile.tempdir = path\n",
    "\n",
    "# tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61fe7db",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "set_tempdir('/scr1/users/dongjp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a047a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrodeMetadata:\n",
    "\n",
    "    def __init__(self, bandpass:list[int]=None, plot_pg=False) -> None:\n",
    "        self.bandpass: list[int] = bandpass if bandpass is not None else constants.GLOBAL_BANDPASS\n",
    "\n",
    "        tetrode = pi.generate_tetrode() # FIXME this should be 25 microns across, default 10\n",
    "        tetrode.set_device_channel_indices(np.arange(4))\n",
    "        tetrode.set_contact_ids(np.arange(4))\n",
    "        pg = pi.ProbeGroup()\n",
    "        pg.add_probe(tetrode)\n",
    "        self.probe_group = pg\n",
    "\n",
    "        if plot_pg:\n",
    "            _, ax2 = plt.subplots(1, 1)\n",
    "            plot_probe(tetrode, ax=ax2, with_device_index=True, with_contact_id=True)\n",
    "            plt.show()\n",
    "    \n",
    "        self.n_channels: int # Will need to be set\n",
    "\n",
    "    def set_n_channels(self, n_channels):\n",
    "        self.n_channels = n_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d552eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "\n",
    "\n",
    "class PyEEGMetadata:\n",
    "    # NOTE this comes from DDFBinaryMetadata from the PyEEG class\n",
    "    def __init__(self, metadata_path, verbose=False) -> None:\n",
    "        self.metadata_path = metadata_path\n",
    "        self.metadata_df = pd.read_csv(metadata_path)\n",
    "        self.verbose = verbose\n",
    "        if verbose > 0:\n",
    "            print(self.metadata_df)\n",
    "\n",
    "        self.n_channels = len(self.metadata_df.index)\n",
    "        self.f_s = self.__getsinglecolval(\"SampleRate\")\n",
    "        self.V_units = self.__getsinglecolval(\"Units\")\n",
    "        self.mult_to_uV = self.__convert_units_to_multiplier(self.V_units)\n",
    "        self.precision = self.__getsinglecolval(\"Precision\")\n",
    "        self.dt_end: datetime\n",
    "        self.dt_start: datetime\n",
    "        if \"LastEdit\" in self.metadata_df.keys():\n",
    "            self.dt_end = dateutil.parser.isoparse(self.__getsinglecolval(\"LastEdit\"))\n",
    "        else:\n",
    "            self.dt_end = None\n",
    "            warnings.warn(\"No LastEdit column provided in metadata. dt_end set to None\")\n",
    "\n",
    "        self.channel_to_info = self.metadata_df.loc[:, [\"BinColumn\", \"ProbeInfo\"]].set_index('BinColumn').T.to_dict('list')\n",
    "        self.channel_to_info = {k:v[0] for k,v in self.channel_to_info.items()}\n",
    "        self.info_to_channel = {v:k for k,v in self.channel_to_info.items()}\n",
    "        self.id_to_info = {k-1:v for k,v in self.channel_to_info.items()}\n",
    "        self.info_to_id = {v:k for k,v in self.id_to_info.items()}\n",
    "        self.entity_to_info = self.metadata_df.loc[:, [\"Entity\", \"ProbeInfo\"]].set_index('Entity').T.to_dict('list')\n",
    "        self.entity_to_info = {k:v[0] for k,v in self.entity_to_info.items()}\n",
    "        self.channel_infos = list(self.channel_to_info.values())\n",
    "\n",
    "        # TODO read probe geometry information, may be user-defined\n",
    "\n",
    "    def __getsinglecolval(self, colname):\n",
    "        vals = self.metadata_df.loc[:, colname]\n",
    "        if len(np.unique(vals)) > 1:\n",
    "            warnings.warn(f\"Not all {colname}s are equal!\")\n",
    "        if vals.size == 0:\n",
    "            return None\n",
    "        return vals.iloc[0]\n",
    "    \n",
    "    def __convert_units_to_multiplier(self, current_units, target_units='µV'):\n",
    "        units_to_mult = {'µV' : 1e-6,\n",
    "                        'mV' : 1e-3,\n",
    "                        'V' : 1,\n",
    "                        'nV' : 1e-9}\n",
    "        \n",
    "        assert current_units in units_to_mult.keys(), f\"No valid current unit called '{current_units}' found\"\n",
    "        assert target_units in units_to_mult.keys(), f\"No valid target unit called '{target_units}' found\"\n",
    "\n",
    "        return units_to_mult[current_units] / units_to_mult[target_units]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be3e8d5",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "tempmeta = PyEEGMetadata('/mnt/isilon/marsh_single_unit/MarshMountainSort/pyeegbins/1158_Exp_5 half turns_right after turning_11-10-16/1158_Exp_5 half turns_right after turning_11-10-16_Meta.csv')\n",
    "pprint(vars(tempmeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c96cbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess recording for sorting\n",
    "def _prep_rec_sorting(recording: si.BaseRecording, metadata: TetrodeMetadata):\n",
    "    # NOTE comparing this preprocessing pipeline with IBL-like, IBL performs better\n",
    "    # rec_prep = recording.clone()\n",
    "    # rec_prep = spre.common_reference(rec_prep)\n",
    "    # rec_prep = spre.scale(rec_prep, gain=100) # Scaling for whitening\n",
    "    # rec_prep = spre.whiten(rec_prep)\n",
    "    # rec_prep = spre.notch_filter(rec_prep, freq=60, q=30) # Get rid of mains hum\n",
    "    # rec_prep = spre.bandpass_filter(rec_prep, freq_min=metadata.bandpass[0], freq_max=metadata.bandpass[1], ftype='butter')\n",
    "\n",
    "    # REVIEW experimental IBL destriping-like preprocessing\n",
    "    rec_prep = recording.clone()\n",
    "    rec_prep = spre.resample(rec_prep, resample_rate=constants.GLOBAL_F_S)\n",
    "    rec_prep = spre.notch_filter(rec_prep, freq=60, q=30) # Get rid of mains hum\n",
    "    # rec_prep = spre.highpass_filter(rec_prep, freq_min=metadata.bandpass[0])\n",
    "    rec_prep = spre.bandpass_filter(rec_prep, freq_min=metadata.bandpass[0], freq_max=metadata.bandpass[1], ftype='butter')\n",
    "    bad_channel_ids, channel_labels = spre.detect_bad_channels(rec_prep)\n",
    "    print(f\"\\tBad channels: {bad_channel_ids}\")\n",
    "    print(f\"\\tChannel labels: {channel_labels}\")\n",
    "    rec_prep = spre.interpolate_bad_channels(rec_prep, bad_channel_ids=bad_channel_ids)\n",
    "    rec_prep = spre.common_reference(rec_prep, operator='median')\n",
    "    rec_prep = spre.scale(rec_prep, gain=100) # Scaling for whitening\n",
    "    rec_prep = spre.whiten(rec_prep)\n",
    "\n",
    "    # rec_prep = spre.highpass_spatial_filter(rec_prep, n_channel_pad=4, ) # broken for tiny channels with clustered depths, and also not needed\n",
    "    \n",
    "    return rec_prep\n",
    "\n",
    "# Preprocess recording for waveform extraction. Mangles the data less than preprocess_recording_sorting\n",
    "def _prep_rec_waveforms(recording: si.BaseRecording, metadata: TetrodeMetadata):\n",
    "    rec_prep = recording.clone()\n",
    "    rec_prep = spre.notch_filter(rec_prep, freq=60, q=30) # Get rid of mains hum\n",
    "    rec_prep = spre.bandpass_filter(rec_prep, freq_min=metadata.bandpass[0], freq_max=metadata.bandpass[1], ftype='butter')\n",
    "    # rec_prep = spre.highpass_filter(rec_prep, freq_min=metadata.bandpass[0], ftype='butter')\n",
    "    return rec_prep\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __init__(self, silence=True) -> None:\n",
    "        self.silence = silence\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.silence:\n",
    "            self._original_stdout = sys.stdout\n",
    "            sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.silence:\n",
    "            sys.stdout.close()\n",
    "            sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db526d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _move_PyEEG_bin_meta_into_subfolders(datadir:Path, suffix_delimiter='_'):\n",
    "    files = glob.glob( str(datadir / '*.*'))\n",
    "    files.sort()\n",
    "\n",
    "    files_set = [suffix_delimiter.join(Path(x).name.split(suffix_delimiter)[:-1]) for x in files]\n",
    "    files_set = list(set(files_set))\n",
    "\n",
    "    for i, files_set_substring in enumerate(files_set):\n",
    "        print(files_set_substring)\n",
    "        for filename in files:\n",
    "            if files_set_substring in filename:\n",
    "                os.makedirs(datadir / files_set_substring, exist_ok=True)\n",
    "                shutil.move(filename, datadir / files_set_substring)\n",
    "\n",
    "_move_PyEEG_bin_meta_into_subfolders(Path('/mnt/isilon/marsh_single_unit/MarshMountainSort/pyeegbins/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b41868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ILongReader(ABC):\n",
    "    # Extract within one folder, possibly containing many individual files. Have ways to split apart channel positions\n",
    "    # For 1 animal, 1 depth, all recordings at that depth. Stitch together multiple if needed\n",
    "\n",
    "    # REGION_TO_IDX = \n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, data_folder, data_suffix, metadata: TetrodeMetadata) -> None:\n",
    "        self.data_folder = Path(data_folder) # Unique for each depth\n",
    "        self.data_folder_name = self.data_folder.name\n",
    "        self.data_suffix = data_suffix\n",
    "        if metadata is None:\n",
    "            self.metadata = TetrodeMetadata()\n",
    "        else:\n",
    "            self.metadata = metadata\n",
    "        self.longrecording: si.BaseRecording\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_files_in_datafolder(self) -> list[str]:\n",
    "        searchstr = str(self.data_folder / f'*{self.data_suffix}')\n",
    "        files = glob.glob(searchstr)\n",
    "        files.sort()\n",
    "        return files\n",
    "    \n",
    "    # def get_filenames_in_datafolder(self) -> list[str]:\n",
    "    #     return [Path(x).name.partition('.')[0] for x in self.get_files_in_datafolder()]\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_region(self, region_name:str) -> si.BaseRecording:\n",
    "        if region_name not in constants.REGIONS:\n",
    "            raise ValueError(f'Invalid region given {region_name}, pick from {constants.REGIONS}')\n",
    "\n",
    "    def _strip_to_numeric(self, string:str):\n",
    "        return int(re.sub(r'\\D', '', string))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13a0a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongBinaryReader(ILongReader):\n",
    "    \"\"\"\n",
    "    This class is deprecated in favor of LongPyEEGReader\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder, metadata=None) -> None:\n",
    "        super().__init__(data_folder, data_suffix='.npy.gz', metadata=metadata)\n",
    "        self.metadata.set_n_channels(12)\n",
    "        warnings.warn(\"LongBinaryReader is deprecated. Use LongPyEEGReader for analyzing Datawave data\")\n",
    "    \n",
    "    def get_files_in_datafolder(self):\n",
    "        files = super().get_files_in_datafolder()\n",
    "        return [x for x in files if any([reg in x for reg in constants.REGIONS])]\n",
    "    \n",
    "    def load_region(self, region_name: str):\n",
    "        super().load_region(region_name)\n",
    "        if not any([region_name in filename for filename in self.get_files_in_datafolder()]):\n",
    "            return None\n",
    "        region_file = [x for x in self.get_files_in_datafolder() if region_name in x]\n",
    "        # print(region_file)\n",
    "        if len(region_file) > 1:\n",
    "            raise ValueError(f\"Too many regions found {region_name}: {region_file}\")\n",
    "        region_file = region_file[0]\n",
    "        rec = self._read_npygz_as_rec(region_file)\n",
    "        rec = rec.set_probegroup(self.metadata.probe_group)\n",
    "        return rec\n",
    "        \n",
    "    def _read_npygz_as_rec(self, npygz_path):\n",
    "        temppath = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())\n",
    "        with open(temppath, 'wb') as tmp:\n",
    "            fcomp = gzip.GzipFile(npygz_path, 'rb')\n",
    "            binary_decomp = np.load(fcomp)\n",
    "            binary_decomp.tofile(tmp)\n",
    "            rec = se.read_binary(tmp.name, \n",
    "                                 sampling_frequency=25000, # NOTE hardcoded, because PyEEG handles this more elegantly and should be used over this\n",
    "                                 dtype=np.float32,\n",
    "                                 num_channels=self.metadata.n_channels, \n",
    "                                 gain_to_uV=0.04, # REVIEW what is the true gain to uV? datawave says stored as mV, but remember that all the units seemed off by a factor of 1000?\n",
    "                                 offset_to_uV=0)\n",
    "        return rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5083aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongPyEEGReader(ILongReader):\n",
    "    \n",
    "    def __init__(self, data_folder, metadata:TetrodeMetadata=None):\n",
    "        super().__init__(data_folder, data_suffix='', metadata=metadata)\n",
    "\n",
    "        files = self.get_files_in_datafolder()\n",
    "        self.__pyeeg_metadata_path = [x for x in files if '.csv' in x][0]\n",
    "        self._binary_path = [x for x in files if 'ColMajor.bin' in x][0] # NOTE hardcoded check for ColMajor, RowMajor will not be picked up\n",
    "        self.pyeeg_metadata:PyEEGMetadata = PyEEGMetadata(self.__pyeeg_metadata_path)\n",
    "        \n",
    "        self.metadata.set_n_channels(self.pyeeg_metadata.n_channels)\n",
    "        \n",
    "    def get_files_in_datafolder(self):\n",
    "        return super().get_files_in_datafolder()\n",
    "    \n",
    "    def load_region(self, region_name:str, region_to_channel:dict = None, corrective_mult:float = 2e-4):\n",
    "        super().load_region(region_name)\n",
    "        if region_to_channel is None:\n",
    "            region_to_channel = constants.REGION_TO_DATAWAVE_CHANNEL\n",
    "        if region_name not in region_to_channel.keys():\n",
    "            return None\n",
    "        channels = region_to_channel[region_name]\n",
    "        if not hasattr(self, '_rec'):\n",
    "            col_bin = np.fromfile(self._binary_path, dtype=self.pyeeg_metadata.precision)\n",
    "            row_bin = np.reshape(col_bin, (-1, self.pyeeg_metadata.n_channels), order='F')\n",
    "\n",
    "            si_params = {\"sampling_frequency\" : self.pyeeg_metadata.f_s,\n",
    "                        \"dtype\" : self.pyeeg_metadata.precision,\n",
    "                        \"num_channels\" : self.pyeeg_metadata.n_channels,\n",
    "                        \"channel_ids\" : [self.pyeeg_metadata.info_to_id[x] for x in self.pyeeg_metadata.channel_infos],\n",
    "                        \"gain_to_uV\" : self.pyeeg_metadata.mult_to_uV * corrective_mult, # REVIEW corrective multiplier unknown\n",
    "                        \"offset_to_uV\" : 0,\n",
    "                        \"time_axis\" : 0,\n",
    "                        \"is_filtered\" : False}\n",
    "\n",
    "            temppath = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())\n",
    "            print(f\"Opening tempfile {temppath}\")\n",
    "            with open(temppath, \"wb\") as tmp:\n",
    "                row_bin.tofile(tmp)\n",
    "                self._rec = se.read_binary(tmp.name, **si_params)\n",
    "        \n",
    "\n",
    "        channel_id_subset = self._rec.channel_ids[channels]\n",
    "        \n",
    "        rec = self._rec.clone()\n",
    "        rec = rec.select_channels(channel_id_subset)\n",
    "        rec = rec.set_probegroup(self.metadata.probe_group)\n",
    "\n",
    "        return rec\n",
    "        \n",
    "    # def _read_npygz_as_rec(self, npygz_path):\n",
    "        # return rec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06deb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongIntanReader(ILongReader):\n",
    "\n",
    "    INTAN_PORTS = ['A', 'B', 'C', 'D']\n",
    "\n",
    "    def __init__(self, data_folder, metadata=None, intan_port='A', all_intan_ports=['A']) -> None:\n",
    "        super().__init__(data_folder, '.rhd', metadata=metadata)\n",
    "        self.metadata.set_n_channels(16) # NOTE hardcoded\n",
    "        self.rec_allport_allregion: si.ConcatenateSegmentRecording = None\n",
    "        self.all_intan_ports = all_intan_ports\n",
    "        if intan_port is None:\n",
    "            self.intan_port = 'A'\n",
    "            warnings.warn(\"Intan port not set, assuming port 'A'\")\n",
    "        else:\n",
    "            self.intan_port = intan_port\n",
    "            if intan_port not in self.INTAN_PORTS:\n",
    "                raise ValueError('Invalid intan_port')\n",
    "            self.data_folder_name = self.__intan_datafoldername_allports_to_oneport()\n",
    "        \n",
    "    def __intan_datafoldername_allports_to_oneport(self, separator='-'):\n",
    "        # From all intan ports, parse out the unused intan port and return the full folder name\n",
    "        keeps = [False] * len(self.all_intan_ports)\n",
    "        idx = self.all_intan_ports.index(self.intan_port)\n",
    "        keeps[idx] = True\n",
    "\n",
    "        splitname = self.data_folder_name.split(separator)\n",
    "        keeps.extend([True] * (len(splitname) - len(keeps)))\n",
    "        out = np.array(splitname)[keeps].tolist()\n",
    "        out = separator.join(out)\n",
    "        return out\n",
    "    \n",
    "    def _get_intan_identifier(self, separator='-'):\n",
    "        splitname = self.data_folder_name.split(separator)\n",
    "        return splitname[0]\n",
    "\n",
    "    \n",
    "    def get_files_in_datafolder(self):\n",
    "        return super().get_files_in_datafolder()\n",
    "      \n",
    "    def load_region(self, region_name: str):\n",
    "        super().load_region(region_name)\n",
    "        if self.rec_allport_allregion is None:\n",
    "            self._load_full_recording()\n",
    "\n",
    "        ids = self.rec_allport_allregion.get_channel_ids()\n",
    "        ids_keep = [self._strip_to_numeric(x) in constants.REGION_TO_INTAN_CHANNEL[region_name] and self.intan_port in x for x in ids]\n",
    "        ids_remove = ~np.array(ids_keep)\n",
    "        rec_oneport_oneregion: si.BaseRecording = self.rec_allport_allregion.remove_channels(ids[ids_remove])\n",
    "        rec_oneport_oneregion.set_probegroup(self.metadata.probe_group, in_place=True)\n",
    "        return rec_oneport_oneregion\n",
    "\n",
    "    def _load_full_recording(self):\n",
    "        all_intan_recs = []\n",
    "        for intan in self.get_files_in_datafolder():\n",
    "            rec = se.read_intan(intan, stream_id='0')\n",
    "            rec = spre.unsigned_to_signed(rec)\n",
    "            all_intan_recs.append(rec)\n",
    "        \n",
    "        self.rec_allport_allregion = si.concatenate_recordings(all_intan_recs)\n",
    "        # self.rec_allport_allregion.set_probegroup(self.metadata.probe_group, in_place=True)\n",
    "        return self.rec_allport_allregion\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c04aea4d",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "# temp = LongPyEEGReader('/mnt/isilon/marsh_single_unit/MarshMountainSort/pyeegbins/1133_WT_5 half turns_after recovery_01-25-17')\n",
    "# rec: si.BaseRecording = temp.load_region('ca3')\n",
    "# temp2 = LongIntanReader('/mnt/isilon/marsh_single_unit/MarshMountainSort/rhds/537-WT-M-5htL-5htR tetrodes 11-11-24_241111_105746')\n",
    "# rec2: si.BaseRecording = temp2.load_region('ca1o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af1fded4",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "print(rec.get_channel_gains())\n",
    "print(rec.get_channel_offsets())\n",
    "print(rec2.get_channel_gains())\n",
    "print(rec2.get_channel_offsets())\n",
    "\n",
    "# print(temp.pyeeg_metadata.mult_to_uV)\n",
    "# traces = rec.get_traces(end_frame=25000 * 600)\n",
    "# print(np.min(traces))\n",
    "# print(np.max(traces))\n",
    "\n",
    "# traces = rec.get_traces(end_frame=25000 * 600, return_scaled=True)\n",
    "# print(np.min(traces))\n",
    "# print(np.max(traces))\n",
    "\n",
    "print(\"\\nground truth:\")\n",
    "traces2 = rec2.get_traces()\n",
    "print(np.min(traces2))\n",
    "print(np.max(traces2))\n",
    "\n",
    "# print(temp2.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44949392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IAnimalAnalyzer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, base_folder:str, identifier:str, datadir_name:str, sortdir_name:str='sortings',\n",
    "                 truncate:bool=False, verbose:bool=True, omit:list[str]=[]) -> None:\n",
    "        self.base_folder = Path(base_folder)\n",
    "        self.identifier = identifier\n",
    "        self.datadir_name = datadir_name\n",
    "        self.sortdir_name = sortdir_name\n",
    "        self.data_folder = self.base_folder / datadir_name\n",
    "        self.sort_folder = self.base_folder / sortdir_name\n",
    "        self.truncate = truncate\n",
    "        self.verbose = verbose\n",
    "        self.omit = omit\n",
    "        self._sorting_parameters = ms5.Scheme2SortingParameters(\n",
    "            phase1_detect_channel_radius=100,\n",
    "            detect_channel_radius=100,\n",
    "            snippet_T1=40,\n",
    "            snippet_T2=40,\n",
    "        )\n",
    "\n",
    "    def _glob_folders(self, base_subdir_name:str, overwrite_identifier=None):\n",
    "        identifier = self.identifier if overwrite_identifier is None else overwrite_identifier\n",
    "        searchstring = self.base_folder / base_subdir_name / f'{identifier}*'\n",
    "        subfolders = glob.glob(str(searchstring))\n",
    "        subfolders = list(set(subfolders))\n",
    "        subfolders = [x for x in subfolders if os.path.isdir(x)]\n",
    "        subfolders = [x for x in subfolders if not any(substring in x for substring in self.omit)]\n",
    "        subfolders.sort()\n",
    "        return subfolders\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11ae9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalSorter(IAnimalAnalyzer):\n",
    "    # Sort one animal, all depths, all regions\n",
    "    # To handle the 1+ intan problem, specify intan_port name and all_intan_ports port order\n",
    "\n",
    "    def __init__(self, base_folder: str, identifier:str, datadir_name: str, sortdir_name: str = 'sortings',\n",
    "                 truncate:bool=False, verbose:bool=True, omit:list[str]=[],\n",
    "                 intan_port:str=None, all_intan_ports:list[str]=None) -> None:\n",
    "        super().__init__(base_folder, identifier, datadir_name, sortdir_name, truncate, verbose, omit)\n",
    "\n",
    "        self.datadir_subfolders = self._glob_folders(self.datadir_name)\n",
    "        if len(self.datadir_subfolders) == 0:\n",
    "            raise ValueError(\"No data folders found\")\n",
    "        self.id_single = None\n",
    "        \n",
    "        dicts:list[dict] = []\n",
    "        for i, folder in enumerate(self.datadir_subfolders):\n",
    "\n",
    "            if self.truncate and i >= 3:\n",
    "                print(\"truncate == True, breaking..\")\n",
    "                break\n",
    "            if self.verbose:\n",
    "                print(f\"[{i+1}/{len(self.datadir_subfolders)}] Reading {folder}..\")\n",
    "            match self.datadir_name:\n",
    "                case 'bin' | 'bins' | 'gzip':\n",
    "                    if intan_port is not None or all_intan_ports is not None:\n",
    "                        warnings.warn('Intan parameters ignored when reading binary files')\n",
    "                    reader = LongBinaryReader(folder)\n",
    "                    self.id_single = self.identifier\n",
    "                case 'pyeegbins':\n",
    "                    reader = LongPyEEGReader(folder)\n",
    "                    self.id_single = self.identifier\n",
    "                case 'rhd' | 'rhds' | 'intan':\n",
    "                    reader = LongIntanReader(folder, intan_port=intan_port, all_intan_ports=all_intan_ports)\n",
    "                    self.id_single = reader._get_intan_identifier()\n",
    "                case _:\n",
    "                    raise ValueError(f\"Invalid data filetype {self.datadir_name}\")\n",
    "            dicts.append({\n",
    "                'folderpath' : reader.data_folder,\n",
    "                'foldername' : reader.data_folder_name,\n",
    "                'identifier' : self.identifier,\n",
    "                'extension' : reader.data_suffix,\n",
    "                'reader' : reader,\n",
    "                # REVIEW Are any of these other features useful at all, besides reader? Possibly for futureproofing\n",
    "            })\n",
    "            \n",
    "        self.df_readers = pd.DataFrame(dicts)\n",
    "        \n",
    "    def delete_sorting_folders(self):\n",
    "        # Deletes sorting folders for a single id. E.g. one port on an intan collection of ports, all binary files with some ID\n",
    "        sort_folders = self._glob_folders(self.sortdir_name, overwrite_identifier=self.id_single)\n",
    "        for folder in sort_folders:\n",
    "            print(f\"Deleting {folder}..\")\n",
    "            shutil.rmtree(folder, ignore_errors=True, onerror=self.__handle_rmtree_error)\n",
    "\n",
    "    def __handle_rmtree_error(self, function, path, excinfo):\n",
    "        print(function, path, excinfo)\n",
    "        time.sleep(3)\n",
    "        shutil.rmtree(path, ignore_errors=True)\n",
    "    \n",
    "    def sort_all(self, **kwargs):\n",
    "        \n",
    "        for i, row in self.df_readers.iterrows():\n",
    "            temp_dir = Path(tempfile.gettempdir()) / os.urandom(24).hex()\n",
    "            os.makedirs(temp_dir)\n",
    "\n",
    "            reader:ILongReader = row['reader']\n",
    "            for region in constants.REGIONS:\n",
    "                if self.verbose:\n",
    "                    print(f\"[{i+1}/{len(self.datadir_subfolders)}] Loading data: ({region}) {reader.data_folder_name}..\")\n",
    "                if reader.data_suffix == '.rhd':\n",
    "                    recording = reader.load_region(region_name=region)\n",
    "                else:\n",
    "                    recording = reader.load_region(region_name=region)\n",
    "                if recording is None:\n",
    "                    print(f\"Missing region {region}, skipping\")\n",
    "                    continue\n",
    "\n",
    "                recording_sorting = _prep_rec_sorting(recording, metadata=reader.metadata) # Preprocess recording\n",
    "                recording_waveforms = _prep_rec_waveforms(recording, metadata=reader.metadata) # Preprocess recording for waveforms\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(f\"[{i+1}/{len(self.datadir_subfolders)}] Sorting..\")\n",
    "                with HiddenPrints(silence=True):\n",
    "                    recording_cached = create_cached_recording(recording_sorting, \n",
    "                                                               folder=temp_dir, \n",
    "                                                               chunk_duration='5s', \n",
    "                                                               n_jobs=kwargs.pop('n_jobs', None)) #ANCHOR testing\n",
    "                    # Extract and sort spikes!\n",
    "                    sorting = ms5.sorting_scheme2(\n",
    "                        recording=recording_cached,\n",
    "                        sorting_parameters=self._sorting_parameters,\n",
    "                    )\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(f\"[{i+1}/{len(self.datadir_subfolders)}] Saving..\")\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings('ignore', message='^.*parallel processing is disabled.*$')\n",
    "                    self._save_sorting(sorting=sorting, recording=recording_waveforms, \n",
    "                                    reader=reader, region=region) # Save sorting\n",
    "                \n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "                \n",
    "    def _save_sorting(self, sorting:si.BaseSorting, recording:si.BaseRecording, reader:ILongReader, \n",
    "                      region:str, rec_name='rec', sort_name='sort'):\n",
    "        sort_subfolder = self.sort_folder / reader.data_folder_name / region\n",
    "        os.makedirs(sort_subfolder, exist_ok=True)\n",
    "        with HiddenPrints(silence=True):\n",
    "            se.NpzSortingExtractor.write_sorting(sorting=sorting, save_path=sort_subfolder / sort_name) # sort.npz\n",
    "            recording.save_to_folder(folder=sort_subfolder / rec_name, overwrite=True) # rec folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31d69598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalSortLoader(IAnimalAnalyzer):\n",
    "    \n",
    "    def __init__(self, base_folder: str, identifier: str, sortdir_name: str = 'sortings', \n",
    "                 truncate: bool = False, verbose: bool = True, omit: list[str]=[]) -> None:\n",
    "        super().__init__(base_folder, identifier, '', sortdir_name, truncate, verbose, omit)\n",
    "        self.sortdir_subfolders = self._glob_folders(sortdir_name)\n",
    "        if len(self.sortdir_subfolders) == 0:\n",
    "            raise ValueError('No sortings found')\n",
    "        self.df_sorts:pd.DataFrame = None\n",
    "    \n",
    "    # Load in all sortings and recordings in the sortings folder\n",
    "    def load_sortings_df(self):\n",
    "        out = []\n",
    "        for subfolder in self.sortdir_subfolders:\n",
    "            for region in constants.REGIONS:\n",
    "                out.append({\n",
    "                    'folderpath' : subfolder,\n",
    "                    'foldername' : Path(subfolder).name,\n",
    "                    'rec' : AnimalSortLoader.load_recording(subfolder, region=region),\n",
    "                    'sort' : AnimalSortLoader.load_sorting(subfolder, region=region),\n",
    "                    'region' : region\n",
    "                })\n",
    "        df = pd.DataFrame(out)\n",
    "        self.df_sorts = df\n",
    "        return df\n",
    "    \n",
    "    def __get_df_sorts(self):\n",
    "        if self.df_sorts is None:\n",
    "            raise TypeError(\"Sorting dataframe was not generated. Run load_sort_df first\")\n",
    "        return self.df_sorts.copy()\n",
    "    \n",
    "    def extract_analyzers_df(self):\n",
    "        df = self.__get_df_sorts()\n",
    "        df['analyze'] = df.apply(self.__extract_sa_row, axis=1)\n",
    "        \n",
    "        self.df_sorts = df\n",
    "        return df\n",
    "\n",
    "    def __extract_sa_row(self, row:pd.Series):\n",
    "        rec:si.BaseRecording = row.rec\n",
    "        sort:si.NpzSortingExtractor = row.sort\n",
    "        if rec is None or sort is None:\n",
    "            warnings.warn(\"rec or sort is None\")\n",
    "            return None\n",
    "        rec = spre.resample(rec, constants.GLOBAL_F_S) # Upsample/downsample recording to global sampling frequency\n",
    "        \n",
    "        sa = si.create_sorting_analyzer(sorting=sort, recording=rec, format='memory', sparse=False)\n",
    "        return sa\n",
    "\n",
    "    def compute_extension_df(self, extension:str, region:str | None, **kwargs):\n",
    "        df = self.__get_df_sorts()\n",
    "        allkwargs = dict(kwargs, extension=extension)\n",
    "        if region is not None:\n",
    "            mask = df['region'] == region\n",
    "            rows = df.loc[mask, 'analyze'].apply(self.__compute_extension_row, **allkwargs)\n",
    "            df.loc[mask, 'analyze'] = rows\n",
    "        else:\n",
    "            df['analyze'].apply(self.__compute_extension_row, **allkwargs)\n",
    "        self.df_sorts = df\n",
    "        return df\n",
    "    \n",
    "    # NOTE Unable to handle specific parameters; better off computing one-by-one with compute_extension_df\n",
    "    def compute_all_extensions_df(self, region:str=None):\n",
    "        df = self.__get_df_sorts()\n",
    "        if region is not None:\n",
    "            mask = df['region'] == region\n",
    "            rows = df.loc[mask, 'analyze'].apply(self.__compute_all_extensions_row)\n",
    "            df.loc[mask, 'analyze'] = rows\n",
    "        else:\n",
    "            df['analyze'].apply(self.__compute_all_extensions_row)\n",
    "        self.df_sorts = df\n",
    "        return df\n",
    "\n",
    "    def __compute_extension_row(self, sa:si.SortingAnalyzer, extension:str, **kwargs):\n",
    "        # print(sa.extensions)\n",
    "        if sa is None:\n",
    "            return\n",
    "        if sa.get_num_units() > 0:\n",
    "            sa.compute_one_extension(extension_name=extension, **kwargs)\n",
    "        return sa\n",
    "    \n",
    "    def __compute_all_extensions_row(self, sa:si.SortingAnalyzer):\n",
    "        if sa is None:\n",
    "            return\n",
    "        if sa.get_num_units() > 0:\n",
    "            exts = sa.get_computable_extensions()\n",
    "            for ext in exts:\n",
    "                sa.compute_one_extension(extension_name=ext)\n",
    "        return sa\n",
    "\n",
    "    def _get_computable_extensions(self, rowidx:int):\n",
    "        row = self.__get_df_sorts().iloc[rowidx]\n",
    "        sa:si.SortingAnalyzer = row.analyze\n",
    "        return sa.get_computable_extensions()\n",
    "\n",
    "    def _get_default_extension_params(self, rowidx:int, extension:str):\n",
    "        row = self.__get_df_sorts().iloc[rowidx]\n",
    "        sa:si.SortingAnalyzer = row.analyze\n",
    "        return sa.get_default_extension_params(extension_name=extension)\n",
    "\n",
    "    def plot_units_oneregion(self, region:str, \n",
    "                             plot_type: constants.DEPTHPLOT_TYPES='heatmap',\n",
    "                             ybound_heatmap=(None, None), xybound_location=(None, None), ybound_wavetemp=(None, None),\n",
    "                             x_nticks=4, max_cols=5,\n",
    "                             cmap_name: Literal['gist_ncar', 'gnuplot2', str] = 'gnuplot2', # FIXME\n",
    "                             order:list[str]=None, titles:list[str]=None,\n",
    "                             save_path:str=None):\n",
    "\n",
    "        df = self.__get_df_sorts()\n",
    "        df = df[df['region'] == region]\n",
    "        if order is not None:\n",
    "            # print(\"dfindex\")\n",
    "            # print(\"\\n\".join(df['foldername']))\n",
    "            # print(\"order\")\n",
    "            # print(\"\\n\".join(order))\n",
    "            df = df.set_index('foldername')\n",
    "            assert set(df.index).issubset(order)\n",
    "            order = [x for x in order if x in df.index]\n",
    "            df = df.loc[order]\n",
    "        n_units = df['analyze'].apply(lambda x: x.get_num_units() if x is not None else 0)\n",
    "        n_plotcols = max(n_units)\n",
    "        n_plotrows = len(df.index)\n",
    "        match n_plotcols:\n",
    "            case _ if n_plotcols > max_cols:\n",
    "                warnings.warn(\"Many units detected, limiting number of columns. Potentially artifacts.\")\n",
    "                n_plotcols = max_cols\n",
    "            case 0:\n",
    "                warnings.warn(\"No units detected in region\")\n",
    "                n_plotcols = 1\n",
    "            case _:\n",
    "                pass\n",
    "        \n",
    "        fig, axes = plt.subplots(n_plotrows, n_plotcols, figsize=(2*n_plotcols, 2*n_plotrows), \n",
    "                                sharex=True, sharey=True, squeeze=False)\n",
    "\n",
    "        for j, (idx, row) in enumerate(df.iterrows()):\n",
    "            sa:si.SortingAnalyzer = row.analyze\n",
    "            n_units = sa.get_num_units() if sa is not None else 0\n",
    "            n_units = min(n_units, max_cols)\n",
    "            firstax = axes[j, 0]\n",
    "\n",
    "            for i in range(n_units):\n",
    "                ax = axes[j, i]\n",
    "                unit_cmap = sw.get_unit_colors(sa, map_name=cmap_name)\n",
    "                match plot_type:\n",
    "                    case 'heatmap':\n",
    "                        sw.plot_unit_waveforms_density_map(sa, \n",
    "                                                        use_max_channel=True, \n",
    "                                                        unit_ids=[sa.unit_ids[i]],\n",
    "                                                        unit_colors=unit_cmap,\n",
    "                                                        ax=ax)\n",
    "                        ax.set_ylabel('')\n",
    "                        ax.set_facecolor('black')\n",
    "                        firstax.set_ylabel(\"uV\") # kind of inefficient here but its ok\n",
    "                        firstax.set_ybound(ybound_heatmap[0], ybound_heatmap[1])\n",
    "                    case 'location':\n",
    "                        sw.plot_spike_locations(sa, \n",
    "                                                unit_ids=[sa.unit_ids[i]], \n",
    "                                                with_channel_ids=True,\n",
    "                                                ax=ax,\n",
    "                                                unit_colors=unit_cmap,\n",
    "                                                plot_legend=False)\n",
    "                        ax.set_aspect('equal')\n",
    "                        # ax.set_facecolor('black')\n",
    "                        ax.set_xlim(xybound_location)\n",
    "                        ax.set_ylim(xybound_location)\n",
    "                        ax.set_xlabel('', fontsize='medium')\n",
    "                        ax.set_ylabel('', fontsize='medium')\n",
    "                        firstax.set_ylabel('$\\mu m$')\n",
    "                    case 'waveform':\n",
    "                        # REVIEW maybe have these parameters outside somewhere, maybe in constants, in case reused\n",
    "                        sw.plot_unit_waveforms(sa,\n",
    "                                                unit_ids=[sa.unit_ids[i]],\n",
    "                                                templates_percentile_shading=None,\n",
    "                                                alpha_waveforms=0.1,\n",
    "                                                unit_colors=unit_cmap,\n",
    "                                                set_title=False,\n",
    "                                                # scalebar=True,\n",
    "                                                scale=1e16, # REVIEW I experimented to get this value\n",
    "                                                same_axis=True,\n",
    "                                                plot_legend=False,\n",
    "                                                ax=ax)\n",
    "                        firstax.set_ybound(ybound_wavetemp[0], ybound_wavetemp[1])\n",
    "                    case 'template':\n",
    "                        sw.plot_unit_templates(sa,\n",
    "                                                unit_ids=[sa.unit_ids[i]],\n",
    "                                                templates_percentile_shading=None,\n",
    "                                                alpha_waveforms=0.5,\n",
    "                                                unit_colors=unit_cmap,\n",
    "                                                set_title=False,\n",
    "                                                # scalebar=True,\n",
    "                                                scale=1e16, # REVIEW I experimented to get this value\n",
    "                                                same_axis=True,\n",
    "                                                plot_legend=False,\n",
    "                                                ax=ax)\n",
    "                        firstax.set_ybound(ybound_wavetemp[0], ybound_wavetemp[1])\n",
    "                    case _:\n",
    "                        raise ValueError(f\"Invalid plot_type {plot_type}\")\n",
    "\n",
    "                n_unitspikes = sa.sorting.count_num_spikes_per_unit()[sa.unit_ids[i]]\n",
    "                ax.set_title(f\"Unit {sa.unit_ids[i]}\", fontsize='medium')\n",
    "                ax.text(0.05, 0.05, f\"n={n_unitspikes} / {round(n_unitspikes/sa.get_total_duration(), 2)} Hz\", \n",
    "                        transform=ax.transAxes, \n",
    "                        ha='left', va='bottom', c='C1', fontsize='small', fontweight='bold')\n",
    "                \n",
    "\n",
    "            for i in range(max(n_units, 1), n_plotcols):\n",
    "                ax = axes[j, i]\n",
    "                ax.set_axis_off()\n",
    "\n",
    "            if n_units == 0:\n",
    "                ax = axes[j, 0]\n",
    "                ax.text(0.5, 0.5, \"No units\", transform=ax.transAxes, ha='center', va='center', fontsize='large', c='white', fontweight='bold')\n",
    "                ax.set_facecolor('black')\n",
    "            # Label bottom row x axis\n",
    "            if j == len(list(df.iterrows())) - 1:\n",
    "                for i in range(n_plotcols):\n",
    "                    match plot_type:\n",
    "                        case 'heatmap':\n",
    "                            axes[j, i].set_xlabel('ms')\n",
    "                            self.__set_xticklabels(ax, sa, x_nticks)\n",
    "                        case 'location':\n",
    "                            axes[j, i].set_xlabel('$\\mu m$')\n",
    "                        case 'waveform':\n",
    "                            pass\n",
    "                        case 'template':\n",
    "                            pass\n",
    "                        case _:\n",
    "                            pass\n",
    "\n",
    "\n",
    "            rec_duration = datetime.timedelta(seconds=sa.get_total_duration()) if sa is not None else 0\n",
    "            ylabel_text = '\\n'.join(wrap(f\"{row.folderpath} {region}\", 40))\n",
    "            if titles is not None:\n",
    "                title_text = f\"{titles[j]}\\n\\n{rec_duration}\"\n",
    "            else:\n",
    "                title_text = f\"{ylabel_text}\\n\\n{rec_duration}\"\n",
    "            firstax.text(-1.6, 0.5, title_text, va='center', ha='center', transform=axes[j, 0].transAxes)\n",
    "\n",
    "        if save_path is not None:\n",
    "            os.makedirs(Path(save_path).parent, exist_ok=True)\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def __set_xticklabels(self, ax, sa, x_nticks):\n",
    "        # sa:si.SortingAnalyzer = sa\n",
    "        if sa is None or sa.get_num_units() == 0:\n",
    "            return\n",
    "        tempex:si.ComputeTemplates = sa.get_extension('templates')\n",
    "        xbounds = np.array([-tempex.nbefore, tempex.nafter]) / sa.sampling_frequency * 1e3\n",
    "        xticks = np.linspace(0, tempex.nbefore + tempex.nafter, x_nticks)\n",
    "        xlabels = np.linspace(xbounds[0], xbounds[1], x_nticks)\n",
    "        ax.set_xticks(xticks, labels=xlabels)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_sorting(sort_subfolder_abspath:str, region:str, sort_name='sort'):\n",
    "        sort_path = Path(sort_subfolder_abspath) / region / sort_name\n",
    "        foundfiles = glob.glob(str(sort_path) + '.*')\n",
    "        match len(foundfiles):\n",
    "            case 1:\n",
    "                return se.NpzSortingExtractor(foundfiles[0])\n",
    "            case 0:\n",
    "                warnings.warn(f\"Does not exist: {sort_path}\")\n",
    "                return None\n",
    "            case _:\n",
    "                raise ValueError(f\"Too many files found {foundfiles}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_recording(sort_subfolder_abspath:str, region:str, rec_name='rec'):\n",
    "        rec_path = Path(sort_subfolder_abspath) / region / rec_name\n",
    "        if os.path.exists(rec_path):\n",
    "            return si.load_extractor(rec_path)\n",
    "        else:\n",
    "            warnings.warn(f\"Does not exist: {rec_path}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_sortinganalyzer(sort_subfolder_abspath:str, region:str, sort_name='sort', rec_name='rec'):\n",
    "        sort = AnimalSortLoader.load_sorting(sort_subfolder_abspath, region, sort_name=sort_name)\n",
    "        if sort is None:\n",
    "            warnings.warn(\"Sorting is None\")\n",
    "            return None\n",
    "        rec = AnimalSortLoader.load_recording(sort_subfolder_abspath, region, rec_name=rec_name)\n",
    "        if rec is None:\n",
    "            warnings.warn(\"Recording is None\")\n",
    "            return None\n",
    "        return si.create_sorting_analyzer(sorting=sort, recording=rec, format='memory', sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81c5708a",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Reading /mnt/isilon/marsh_single_unit/MarshMountainSort/pyeegbins/1132_WT Bl6_5 half turns_after turning_01-18-17..\n",
      "[2/3] Reading /mnt/isilon/marsh_single_unit/MarshMountainSort/pyeegbins/1132_WT Bl6_6 half turns_after turning_01-19-17..\n",
      "[3/3] Reading /mnt/isilon/marsh_single_unit/MarshMountainSort/pyeegbins/1132_WT Bl6_7 half turns_after turning_01-23-17..\n",
      "Deleting /mnt/isilon/marsh_single_unit/MarshMountainSort/sortings/1132_WT Bl6_5 half turns_after turning_01-18-17..\n",
      "Deleting /mnt/isilon/marsh_single_unit/MarshMountainSort/sortings/1132_WT Bl6_6 half turns_after turning_01-19-17..\n",
      "Deleting /mnt/isilon/marsh_single_unit/MarshMountainSort/sortings/1132_WT Bl6_7 half turns_after turning_01-23-17..\n",
      "[1/3] Loading data: (ca3) 1132_WT Bl6_5 half turns_after turning_01-18-17..\n",
      "Opening tempfile /scr1/users/dongjp/19c6096750fd72efe3148db589aaa05fa34f6b8dc8b289ba\n",
      "\tBad channels: []\n",
      "\tChannel labels: ['good' 'good' 'good' 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording:   0%|          | 0/282 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Sorting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 282/282 [00:17<00:00, 15.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|##########| 1407/1407 [00:30<00:00, 45.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Loading data: (ca3o) 1132_WT Bl6_5 half turns_after turning_01-18-17..\n",
      "Missing region ca3o, skipping\n",
      "[1/3] Loading data: (ca1s) 1132_WT Bl6_5 half turns_after turning_01-18-17..\n",
      "\tBad channels: []\n",
      "\tChannel labels: ['good' 'good' 'good' 'good']\n",
      "[1/3] Sorting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|##########| 282/282 [00:31<00:00,  9.07it/s]\n",
      "write_binary_recording:   0%|          | 0/1407 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|##########| 1407/1407 [00:30<00:00, 46.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Loading data: (ca1o) 1132_WT Bl6_5 half turns_after turning_01-18-17..\n",
      "\tBad channels: []\n",
      "\tChannel labels: ['good' 'good' 'good' 'good']\n",
      "[1/3] Sorting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|##########| 282/282 [00:37<00:00,  7.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# ao = AnimalSorter('/mnt/isilon/marsh_single_unit/MarshMountainSort', identifier='1269', datadir_name='pyeegbins', truncate=True)\n",
    "# ao.delete_sorting_folders()\n",
    "# ao.sort_all(**constants.FAST_JOB_KWARGS)\n",
    "\n",
    "ao = AnimalSorter('/mnt/isilon/marsh_single_unit/MarshMountainSort', identifier='1132', datadir_name='pyeegbins', truncate=True)\n",
    "ao.delete_sorting_folders()\n",
    "ao.sort_all(**constants.FAST_JOB_KWARGS)\n",
    "\n",
    "# ao = AnimalSorter('/mnt/isilon/marsh_single_unit/MarshMountainSort/', datadir_name='rhds', identifier='579-580',\n",
    "#                   intan_port='A', all_intan_ports=['A', 'B'])\n",
    "# ao.delete_sorting_folders()\n",
    "# ao.sort_all()\n",
    "\n",
    "# ao = AnimalSorter('/mnt/isilon/marsh_single_unit/MarshMountainSort/', datadir_name='rhds', identifier='501-502',\n",
    "#                   truncate=False,\n",
    "#                   intan_port='B', all_intan_ports=['A', 'B'],)\n",
    "# ao.delete_sorting_folders()\n",
    "# ao.sort_all()\n",
    "\n",
    "# ao = AnimalSorter('/mnt/isilon/marsh_single_unit/MarshMountainSort/', datadir_name='rhds', identifier='513-514',\n",
    "#                   truncate=False,\n",
    "#                   intan_port='A', all_intan_ports=['A', 'B'],\n",
    "#                   omit=['14htL'])\n",
    "# display(ao.df_readers)\n",
    "# testvar:LongIntanReader = ao.df_readers.iloc[0]['reader']\n",
    "# rec: si.BaseRecording = testvar.load_region('ca1o')\n",
    "\n",
    "\n",
    "# raise Exception('halt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea466d31",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "asl = AnimalSortLoader('/mnt/isilon/marsh_single_unit/MarshMountainSort/', identifier='1132')\n",
    "region = 'ca1o'\n",
    "\n",
    "asl.load_sortings_df()\n",
    "asl.extract_analyzers_df()\n",
    "\n",
    "asl.compute_extension_df(extension='random_spikes', region=region, max_spikes_per_unit=1000)\n",
    "asl.compute_extension_df(extension='waveforms', region=region, ms_before=1.0, ms_after=2.0)\n",
    "asl.compute_extension_df(extension='templates', region=region, ms_before=1.0, ms_after=2.0)\n",
    "asl.compute_extension_df(extension='spike_locations', region=region, ms_before=0.5, ms_after=0.5, method='center_of_mass')\n",
    "asl.compute_extension_df(extension='principal_components', region=region)\n",
    "\n",
    "# asl.compute_extension_df(extension='spike_locations', region=region, ms_before=1.0, ms_after=2.0, method='monopolar_triangulation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34128ea9",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "# asl._get_default_extension_params(1, 'spike_locations')\n",
    "print(asl._get_default_extension_params(0, 'random_spikes'))\n",
    "print(asl._get_default_extension_params(0, 'waveforms'))\n",
    "print(asl._get_default_extension_params(0, 'templates'))\n",
    "print(asl._get_default_extension_params(0, 'spike_locations'))\n",
    "print(asl._get_default_extension_params(0, 'principal_components'))\n",
    "print(asl._get_default_extension_params(0, 'spike_amplitudes'))\n",
    "print(asl._get_default_extension_params(0, 'noise_levels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "008e2bf1",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "asl.plot_units_oneregion(region=region, xybound_location=(-30, 30), \n",
    "                         plot_type='location')\n",
    "asl.plot_units_oneregion(region=region, ybound_heatmap=(-200, 100),\n",
    "                         plot_type='heatmap')\n",
    "asl.plot_units_oneregion(region=region, ybound_wavetemp=(-40, 40),\n",
    "                         plot_type='waveform')\n",
    "asl.plot_units_oneregion(region=region, ybound_wavetemp=(-40, 40),\n",
    "                         plot_type='template')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "233e5467",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "# 1211, 766\n",
    "for ident in [501, 502, 513, 514, 529, 530, 537]:\n",
    "    asl = AnimalSortLoader('/mnt/isilon/marsh_single_unit/MarshMountainSort/', identifier=ident)\n",
    "    regions = ['ca3', 'ca3o', 'ca1s', 'ca1o']\n",
    "\n",
    "    for region in regions:\n",
    "\n",
    "        asl.load_sortings_df()\n",
    "        asl.extract_analyzers_df()\n",
    "        # temp = ap._get_computable_extensions(0)\n",
    "        # for t in temp:\n",
    "        #     print(f\"{t} : {ap._get_default_extension_params(0, t)}\")\n",
    "        # break\n",
    "\n",
    "        asl.compute_extension_df(extension='random_spikes', region=region, max_spikes_per_unit=1000)\n",
    "        asl.compute_extension_df(extension='waveforms', region=region, ms_before=1.0, ms_after=2.0)\n",
    "        asl.compute_extension_df(extension='templates', region=region, ms_before=1.0, ms_after=2.0)\n",
    "        # asl.compute_extension_df(extension='principal_components', region=region)\n",
    "        # asl.compute_extension_df(extension='spike_locations', region=region, ms_before=1.0, ms_after=2.0, method='monopolar_triangulation')\n",
    "        asl.compute_extension_df(extension='spike_locations', region=region, ms_before=0.5, ms_after=0.5, method='center_of_mass')\n",
    "\n",
    "\n",
    "        # asl.plot_units_oneregion(region=region, ybound_heatmap=(-200, 100), max_cols=10)\n",
    "        tempsavepath = asl.base_folder / 'output-depthsort' / 'New Anims 12-2-24'\n",
    "        asl.plot_units_oneregion(region=region, xybound_location=(-30, 30), \n",
    "                                plot_type='location', save_path=tempsavepath / f'{asl.identifier}_{region}_location')\n",
    "        asl.plot_units_oneregion(region=region, ybound_heatmap=(-200, 100),\n",
    "                                plot_type='heatmap', save_path=tempsavepath / f'{asl.identifier}_{region}_heatmap')\n",
    "        asl.plot_units_oneregion(region=region, ybound_wavetemp=(-40, 40),\n",
    "                                plot_type='waveform', save_path=tempsavepath / f'{asl.identifier}_{region}_waveform')\n",
    "        asl.plot_units_oneregion(region=region, ybound_wavetemp=(-40, 40),\n",
    "                                plot_type='template', save_path=tempsavepath / f'{asl.identifier}_{region}_template')\n",
    "\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19411cb3-264b-4f5a-943b-e3ca43d7c853",
   "metadata": {},
   "source": [
    "## Find All Files for Animal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522c8a1-66e2-4a1e-9f37-ed498f3e0fb7",
   "metadata": {},
   "source": [
    "## Sort Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eb6d7c3-2190-4ddc-8a4e-dc59ac9d6330",
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "# Preprocess recording for sorting\n",
    "def preprocess_recording_sorting(rec, mcorr_folder=None):\n",
    "    rec_prep = rec\n",
    "    rec_prep = spre.common_reference(rec_prep, dtype=np.float32)\n",
    "    rec_prep = spre.scale(rec_prep, gain=10, dtype=np.float32) # Scaling for whitening to work properly\n",
    "    rec_prep = spre.whiten(rec_prep, dtype=np.float32)\n",
    "    rec_prep = spre.bandpass_filter(rec_prep, freq_min=freq_min, freq_max=freq_max, ftype='bessel', dtype=np.float32)\n",
    "    \n",
    "    if mcorr_folder is not None:\n",
    "        motion_info = spre.load_motion_info(mcorr_folder)\n",
    "        rec_prep = sc.motion_interpolation.interpolate_motion(\n",
    "                  recording=rec_prep,\n",
    "                  motion=motion_info['motion'],\n",
    "                  temporal_bins=motion_info['temporal_bins'],\n",
    "                  spatial_bins=motion_info['spatial_bins'],\n",
    "                  **motion_info['parameters']['interpolate_motion_kwargs'])\n",
    "        \n",
    "    return rec_prep\n",
    "\n",
    "# Preprocess recording for waveform extraction. Mangles the data less than preprocess_recording_sorting\n",
    "def preprocess_recording_waveforms(rec):\n",
    "    rec_prep = rec\n",
    "    rec_prep = spre.notch_filter(rec_prep, freq=60, dtype=np.float32) # Get rid of mains hum\n",
    "    rec_prep = spre.highpass_filter(rec_prep, freq_min=60, ftype='bessel', dtype=np.float32)\n",
    "    # rec_prep = spre.highpass_filter(rec_prep, freq_min=60, ftype='butter', dtype=np.float32)\n",
    "    \n",
    "    return rec_prep\n",
    "\n",
    "# Preprocess recording to generate motion correction info.\n",
    "# Will not be used for final sorting, but you can calibrate final sorting with this motion correction info\n",
    "def preprocess_recording_mcorr(rec, mcorr_folder):\n",
    "    rec_prep = rec\n",
    "    rec_prep = spre.common_reference(rec_prep, dtype=np.float32)\n",
    "    rec_prep = spre.bandpass_filter(rec_prep, freq_min=freq_min, freq_max=freq_max, dtype=np.float32)\n",
    "    rec_prep = spre.correct_motion(rec_prep, preset='nonrigid_accurate', folder=mcorr_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
